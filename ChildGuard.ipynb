{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1caa18-4e9b-45fe-b4fd-a7d2ab4aa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3473b0-489e-4d4c-9de0-4bd14c9e3651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kanth\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kanth\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths to your saved models\n",
    "AGE_MODEL_PATH      = \"child_detection.keras\"\n",
    "CRIMINAL_MODEL_PATH = \"criminal_detection.keras\"\n",
    "\n",
    "# Load the models once at import\n",
    "age_model      = load_model(AGE_MODEL_PATH)\n",
    "criminal_model = load_model(CRIMINAL_MODEL_PATH)\n",
    "\n",
    "# Criminal class names must match the training order\n",
    "CRIMINAL_CLASSES = ['Srinivas']  # extend this list if you have more classes\n",
    "\n",
    "# Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "# Image size used during training\n",
    "IMG_SIZE = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c24b7c-5f03-41f8-ad4e-43bc44940b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_face_frame(frame):\n",
    "    \"\"\"\n",
    "    Detects faces in `frame`, classifies each as child or criminal,\n",
    "    draws colored boxes and labels, and returns:\n",
    "      - annotated_frame: the frame with overlays\n",
    "      - child_boxes: list of (x, y, w, h) for faces classified as children\n",
    "    \"\"\"\n",
    "    annotated = frame.copy()\n",
    "    gray = cv2.cvtColor(annotated, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    child_boxes = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = annotated[y:y+h, x:x+w]\n",
    "        # Resize to the expected input shape for the age model (200x200)\n",
    "        face = cv2.resize(roi, (200, 200)).astype(\"float32\") / 255.0\n",
    "        face_batch = np.expand_dims(face, axis=0)\n",
    "\n",
    "        # 1) Age estimation\n",
    "        age_prob = age_model.predict(face_batch, verbose=0)[0][0]\n",
    "        is_child = (age_prob >= 0.1)\n",
    "\n",
    "        if is_child:\n",
    "            label = \"Child\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "            child_boxes.append((x, y, w, h))\n",
    "        else:\n",
    "            # 2) Criminal detection\n",
    "            crim_probs = criminal_model.predict(face_batch, verbose=0)[0]\n",
    "            crim_idx   = np.argmax(crim_probs)\n",
    "            crim_conf  = crim_probs[crim_idx]\n",
    "\n",
    "            if crim_conf == 1.0:\n",
    "                label = CRIMINAL_CLASSES[crim_idx]\n",
    "                color = (0, 0, 255)  # Red\n",
    "            else:\n",
    "                continue  # Neither child nor known criminal\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(annotated, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(annotated, label, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    return annotated, child_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecff260-de88-4917-ad5c-468e76d0cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your trained activity model\n",
    "ACTIVITY_MODEL_PATH = \"activity_monitoring.keras\"\n",
    "activity_model = load_model(ACTIVITY_MODEL_PATH)\n",
    "\n",
    "# Activity labels (order must match training)\n",
    "CLASS_LABELS = ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n",
    "\n",
    "# Define normal and abnormal activities\n",
    "NORMAL_ACTIVITIES = ['CricketShot', 'PlayingCello', 'TennisSwing']\n",
    "ABNORMAL_ACTIVITIES = ['Punch', 'ShavingBeard']\n",
    "\n",
    "\n",
    "def preprocess_video_frames(frames, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses the list of frames to make them compatible with the model.\n",
    "    Resizes and normalizes the frames before feeding them into the model.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for frame in frames:\n",
    "        resized = cv2.resize(frame, target_size)\n",
    "        normalized = resized.astype(\"float32\") / 255.0\n",
    "        processed.append(normalized)\n",
    "    return np.array(processed)\n",
    "\n",
    "\n",
    "def predict_activity_from_frames(frames):\n",
    "    \"\"\"\n",
    "    Predicts the activity based on the most recent frames.\n",
    "    Returns:\n",
    "      - activity label\n",
    "      - status ('Normal' or 'Abnormal')\n",
    "      - color (for display)\n",
    "    \"\"\"\n",
    "    if len(frames) < 20:\n",
    "        return \"Unknown\", \"Insufficient frames\", (0, 0, 255)\n",
    "\n",
    "    sampled_frames = frames[-20:]  # Last 20 frames only\n",
    "    processed = preprocess_video_frames(sampled_frames, target_size=(64, 64))\n",
    "    input_data = np.expand_dims(processed, axis=0)  # Shape: (1, 20, 64, 64, 3)\n",
    "\n",
    "    predictions = activity_model.predict(input_data, verbose=0)[0]\n",
    "    pred_index = np.argmax(predictions)\n",
    "    label = CLASS_LABELS[pred_index]\n",
    "\n",
    "    if label in NORMAL_ACTIVITIES:\n",
    "        return label, \"Normal\", (0, 255, 0)  # Green\n",
    "    else:\n",
    "        return label, \"Abnormal\", (0, 0, 255)  # Red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e09650-99c0-4fea-8e8b-d9eb38c9c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "MODEL_PATH = \"behaviour_analysis.keras\"\n",
    "\n",
    "# Load the trained behavior model\n",
    "behavior_model = load_model(MODEL_PATH)\n",
    "\n",
    "def predict_behavior(frame):\n",
    "    \"\"\"\n",
    "    Predicts if the behavior in a given frame is normal or abnormal.\n",
    "    Returns the label and the color (green/red) for bounding box.\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    normalized = resized.astype('float32') / 255.0\n",
    "    img = np.expand_dims(normalized, axis=0)\n",
    "    \n",
    "    prediction = behavior_model.predict(img, verbose=0)[0][0]\n",
    "\n",
    "    if prediction <= 0.9:\n",
    "        return \"Normal\", (0, 255, 0)  # Green\n",
    "    else:\n",
    "        return \"Abnormal\", (0, 0, 255)  # Red\n",
    "\n",
    "def analyze_behavior_on_frame(frame, detected_children):\n",
    "    \"\"\"\n",
    "    Applies behavior analysis on all detected children in the frame.\n",
    "    `detected_children` is a list of (x, y, w, h) for each child.\n",
    "    \"\"\"\n",
    "    for (x, y, w, h) in detected_children:\n",
    "        child_face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        if child_face.size == 0:\n",
    "            continue\n",
    "\n",
    "        label, color = predict_behavior(child_face)\n",
    "\n",
    "        # Draw rectangle around the face (already drawn in previous step, optional here)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        # Show \"Normal\" or \"Abnormal\" label INSIDE the box (bottom-left)\n",
    "        text_position = (x + 5, y + h - 10)  # Slight padding from bottom-left\n",
    "        cv2.putText(frame, label, text_position, cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, color, 2)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3d0aa1-4a63-45c1-a039-66034719cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"👋 Welcome to ChildGuard\")\n",
    "    print(\"Enter input mode: 'c' for Camera, 'v' for Video path\")\n",
    "    print(\"Press 'q' to quit anytime ❌\")\n",
    "\n",
    "    user_input = input(\"Your choice: \").strip().lower()\n",
    "\n",
    "    if user_input == 'q':\n",
    "        print(\"👋 Quitting the program. Stay safe!\")\n",
    "        return\n",
    "\n",
    "    cap = None\n",
    "    activity_frames = []\n",
    "\n",
    "    if user_input == 'v':\n",
    "        video_path = input(\"🎞️ Enter video file path: \").strip()\n",
    "\n",
    "        # Sanitize path\n",
    "        video_path = video_path.strip('\"').strip(\"'\").replace(\"\\\\\", \"/\")\n",
    "        print(f\"🧾 Cleaned path: {video_path}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"⚠️ Error: Cannot open the video file. Switching to camera 🎥.\")\n",
    "            cap = cv2.VideoCapture(0)\n",
    "        else:\n",
    "            print(\"Video loaded successfully! 🎬\")\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        print(\"📸 Starting camera feed...\")\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Error: Could not access camera or video.\")\n",
    "        return\n",
    "\n",
    "    print(\"🚀 Processing started. Press 'q' to quit early.\")\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"📭 End of stream or can't read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Store frames for activity detection\n",
    "        activity_frames.append(frame)\n",
    "\n",
    "        # Process frame using face recognition (from ChildGuard)\n",
    "        annotated_frame, child_boxes = process_face_frame(frame)\n",
    "\n",
    "        # Process activity recognition from stored frames\n",
    "        activity_label, activity_status, activity_color = predict_activity_from_frames(activity_frames)\n",
    "\n",
    "        # Add activity label to the annotated frame\n",
    "        cv2.putText(annotated_frame, f\"Activity: {activity_label} ({activity_status})\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, activity_color, 2)\n",
    "\n",
    "        # Apply behavior analysis\n",
    "        final_frame = analyze_behavior_on_frame(annotated_frame, child_boxes)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow('ChildGuard - Press q to quit', final_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"👋 Quitting early by user. See you next time!\")\n",
    "            break\n",
    "\n",
    "        if time.time() - start_time > 30:\n",
    "            print(\"⏱️ 30 seconds completed. Shutting down gracefully 💤\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"🧹 Resources cleaned up. Goodbye! 👋\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a3c5a7-fb90-4cae-a2c7-ddeb5fc9cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Welcome to ChildGuard\n",
      "Enter input mode: 'c' for Camera, 'v' for Video path\n",
      "Press 'q' to quit anytime ❌\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your choice:  v\n",
      "🎞️ Enter video file path:  \"E:\\6. WIN SEM -- 2024-2025\\F1. DL\\Child Surveillance DL Project\\2.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Cleaned path: E:/6. WIN SEM -- 2024-2025/F1. DL/Child Surveillance DL Project/2.mp4\n",
      "Video loaded successfully! 🎬\n",
      "🚀 Processing started. Press 'q' to quit early.\n",
      "⏱️ 30 seconds completed. Shutting down gracefully 💤\n",
      "🧹 Resources cleaned up. Goodbye! 👋\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ea9ce-cab6-4d60-ac03-3fecd03e4bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
